{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"8qdzhBI7bvgm"},"source":["# Test Code for Registration and Synthesis"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XCKYSEIuiC3k"},"source":["## Install Packages "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gofN69eriJkF"},"outputs":[],"source":["!pip install antspyx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_percFvSigy1"},"outputs":[],"source":["!pip insatll SimpleITK"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DXY0YB-hGOY"},"outputs":[],"source":["from numpy import spacing\n","import ants\n","import os\n","import torch\n","import shutil\n","import SimpleITK as sitk\n","import nibabel as nib\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import Compose\n","import time\n","import matplotlib.pyplot as plt\n","import glob\n","from tqdm import tqdm\n","import matplotlib\n","from scipy.ndimage import zoom\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qPkcysPJcgDC"},"source":["## Task A: Skull Stripping"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ods2OgNZeqXg"},"source":["### Clone this repository:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_iZje-FckTs"},"outputs":[],"source":["!git clone https://github.com/MIC-DKFZ/HD-BET.git"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cP7Q1yCles-_"},"source":["### Go into the repository (the folder with the setup.py file) and install:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvQp8pLtcs_m"},"outputs":[],"source":["!cd HD-BET"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lf_25XhYcwvt"},"outputs":[],"source":["!pip install -e ."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TXIDZnCMc8mI"},"source":["### Run your dataset to remove the skull region:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOsc-2ZVczpI"},"outputs":[],"source":["'''\n","INPUT_FOLDER: Raw subject folder path (need to remove skull region);\n","OUTPUT_FOLDER: Skull stripping subject folder path.\n","'''\n","!hd-bet -i INPUT_FOLDER -o OUTPUT_FOLDER"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ksFFfO4Gb1WS"},"source":["## Task B: Registration"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MOAVvuiaf74F"},"source":["### Rigid register T2w to T1w, rigid + nonrigid register FA, ADC to T1w"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":256,"status":"ok","timestamp":1684005696347,"user":{"displayName":"Zhiwei GONG","userId":"02116542019368403549"},"user_tz":240},"id":"kkEttv5tb0wv"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Give the input and output folder path\n","input_path = '/skull_removed_folder_path'\n","output_path = '/output_path'\n","\n","if not os.path.exists(output_path):\n","  os.makedirs(output_path)\n","\n","# Define the paths to the input images\n","t1_path = f\"{input_path}/T1w_1mm.nii.gz\"\n","t2_path = f\"{input_path}/T2w_1mm_noalign.nii.gz\"\n","fa_path = f\"{input_path}/FA_deformed.nii.gz\"\n","adc_path = f\"{input_path}/ADC_deformed.nii.gz\"\n","\n","# Load the input images using ANTs\n","t1 = ants.image_read(t1_path)\n","t2 = ants.image_read(t2_path)\n","fa = ants.image_read(fa_path)\n","adc = ants.image_read(adc_path)\n","\n","# Rigid align T2w, FA, and ADC to T1w using ANTs\n","t2_rigid = ants.registration(t1, t2, type_of_transform='Rigid', cost_function='MutualInformation', device=device)\n","fa_rigid = ants.registration(t1, fa, type_of_transform='Rigid', cost_function='MutualInformation', device=device)\n","adc_rigid = ants.registration(t1, adc, type_of_transform='Rigid', cost_function='MutualInformation', device=device)\n","\n","# Non-rigid align FA and ADC to T1w using ANTs\n","fa_nonrigid = ants.registration(t1, fa_rigid['warpedmovout'], type_of_transform='SyN', device=device)\n","adc_nonrigid = ants.registration(t1, adc_rigid['warpedmovout'], type_of_transform='SyN', device=device)\n","\n","# Save the resulting non-rigid registered images\n","ants.image_write(t2_rigid['warpedmovout'], f\"{output_path}/T2w_registered.nii.gz\")\n","ants.image_write(fa_nonrigid['warpedmovout'], f\"{output_path}/FA_registered.nii.gz\")\n","ants.image_write(adc_nonrigid['warpedmovout'], f\"{output_path}/ADC_registered.nii.gz\")\n","shutil.copy(t1_path, f\"{output_path}/T1w_1mm.nii.gz\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sXXSj4y8gDCb"},"source":["### Resample registered FA and ADC dimension from (182, 218, 18) to (145, 174, 145), and resolution from 1mm to 1.25 mm"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"M3rdQCF8gapk"},"source":["#### First use ANTs resample method - change dimension "]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1684006117706,"user":{"displayName":"Zhiwei GONG","userId":"02116542019368403549"},"user_tz":240},"id":"OCtQ0vS-gY6U"},"outputs":[],"source":["if not os.path.exists(output_path):\n","  os.makedirs(output_path)\n","\n","FA_resampled_image = ants.resample_image(fa_nonrigid['warpedmovout'], (145, 174, 145), True, 4) \n","ADC_resampled_image = ants.resample_image(adc_nonrigid['warpedmovout'], (145, 174, 145), True, 4) \n","\n","# Save the ants resampled image\n","ants.image_write(FA_resampled_image, f\"{output_path}/FA_ants_resample.nii.gz\")\n","ants.image_write(ADC_resampled_image, f\"{output_path}/ADC_ants_resample.nii.gz\")\n","shutil.copy(f\"{output_path}/T2w_registered.nii.gz\", f\"{output_path}/T2w_align.nii.gz\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eaf5gSQ6ilM1"},"source":["#### Second use SimpleITK resample method - change the resolution"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1684006689728,"user":{"displayName":"Zhiwei GONG","userId":"02116542019368403549"},"user_tz":240},"id":"6FuNrlPgivQa"},"outputs":[],"source":["submit_path = '/registration_submit_path'\n","\n","if not os.path.exists(submit_path):\n","  os.makedirs(submit_path)\n","\n","FA_file_reader = sitk.ImageFileReader()\n","FA_file_reader.SetImageIO('NiftiImageIO')\n","FA_file_reader.SetFileName(f\"{output_path}/FA_ants_resample.nii.gz\") # need to change path\n","FA_image = FA_file_reader.Execute()\n","\n","ADC_file_reader = sitk.ImageFileReader()\n","ADC_file_reader.SetImageIO('NiftiImageIO')\n","ADC_file_reader.SetFileName(f\"{output_path}/ADC_ants_resample.nii.gz\") # need to change path\n","ADC_image = ADC_file_reader.Execute()\n","\n","FA_image.SetSpacing([1.25,1.25,1.25])\n","ADC_image.SetSpacing([1.25,1.25,1.25])\n","\n","# # Save the SimpleITK resampled image\n","sitk.WriteImage(FA_image, f\"{submit_path}/FA_align.nii.gz\")\n","sitk.WriteImage(ADC_image, f\"{submit_path}/ADC_align.nii.gz\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZPG1NLyEkzyq"},"source":["## Task C: Synthesis"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_S-WSYYWk5gi"},"source":["### Padding T1w_1mm, T2w_registered to (192, 224, 192) for our Network"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":377,"status":"ok","timestamp":1684006932813,"user":{"displayName":"Zhiwei GONG","userId":"02116542019368403549"},"user_tz":240},"id":"6h1NDk3Rk37w"},"outputs":[],"source":["t1_path = f\"{output_path}/T1w_1mm.nii.gz\"\n","t2_path = f\"{output_path}/T2w_registered.nii.gz\"\n","\n","# Load the MRI T1w, T2w images\n","t1w_image = nib.load(t1_path).get_fdata()\n","t2w_image = nib.load(t2_path).get_fdata()\n","\n","# Get the current shape of the images\n","t1w_shape = t1w_image.shape\n","t2w_shape = t2w_image.shape\n","\n","# Calculate the amount of padding needed for each dimension\n","t1w_padding = tuple((32 - dim % 32) % 32 for dim in t1w_shape)\n","t2w_padding = tuple((32 - dim % 32) % 32 for dim in t2w_shape)\n","\n","# Pad the images\n","t1w_padded = np.pad(t1w_image, ((0, t1w_padding[0]), (0, t1w_padding[1]), (0, t1w_padding[2])), mode='constant')\n","t2w_padded = np.pad(t2w_image, ((0, t2w_padding[0]), (0, t2w_padding[1]), (0, t2w_padding[2])), mode='constant')\n","\n","# Create new Nifti images with the padded data and affine information\n","t1w_padded_nii = nib.Nifti1Image(t1w_padded, nib.load(t1_path).affine)\n","t2w_padded_nii = nib.Nifti1Image(t2w_padded, nib.load(t2_path).affine)\n","\n","# Save the padded images in nii.gz format in the patient folder\n","nib.save(t1w_padded_nii, f\"{output_path}/T1w_1mm_padded.nii.gz\")\n","nib.save(t2w_padded_nii, f\"{output_path}/T2w_registered_padded.nii.gz\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vcdcl2JQmpsd"},"source":["### Define the Network - UNet "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7lKFaIfm4In"},"outputs":[],"source":["def conv_block(in_channels, out_channels):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True)\n","    )\n","\n","class UNet(nn.Module):\n","    def __init__(self):\n","        super(UNet, self).__init__()\n","        self.encoder = nn.ModuleList([\n","            conv_block(2, 16),\n","            conv_block(16, 32),\n","            conv_block(32, 64),\n","            conv_block(64, 128)\n","        ])\n","        self.pool = nn.MaxPool2d(2)\n","        self.bridge = conv_block(128, 256)\n","        self.decoder = nn.ModuleList([\n","            conv_block(256, 128),\n","            conv_block(128, 64),\n","            conv_block(64, 32),\n","            conv_block(32, 16)\n","        ])\n","        self.upconv = nn.ModuleList([\n","            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n","            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n","            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n","            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n","        ])\n","        self.final = nn.Conv2d(16, 1, kernel_size=1)\n","\n","    def forward(self, x):\n","        skips = []\n","        for layer in self.encoder:\n","            x = layer(x)\n","            skips.append(x)\n","            x = self.pool(x)\n","        x = self.bridge(x)\n","        for i, layer in enumerate(self.decoder):\n","            x = self.upconv[i](x)\n","            x = torch.cat([x, skips[-(i+1)]], axis=1)\n","            x = layer(x)\n","        x = self.final(x)\n","        return x"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"V4Vp7ohhoWqE"},"source":["### Generate the Test Dataloader - you should run with 1 T1w and 1 T2w at a time"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":152,"status":"ok","timestamp":1684007130175,"user":{"displayName":"Zhiwei GONG","userId":"02116542019368403549"},"user_tz":240},"id":"JJRTNWlhoabk"},"outputs":[],"source":["'''image_proc_2C is for generating 2-channel source images pair (T1w, T2w) as the input to the network.'''\n","def image_proc_2C(filepath1, filepath2):\n","    \"\"\" Data loader (*.nii)\n","    :param filepath1: file path to the first set of images\n","    :param filepath2: file path to the second set of images\n","    :return: 2D array images with 2 channels\n","    \"\"\"\n","    img_data0 = []\n","    img_data1 = []\n","\n","    for item1, item2 in tqdm(zip(sorted(filepath1), sorted(filepath2)), desc='Processing'):\n","        # loading images\n","        img1 = nib.load(item1).get_fdata()\n","        img2 = nib.load(item2).get_fdata()\n","        # stack the two images along the last axis to create a 2-channel image\n","        combined_img = np.stack((img1, img2), axis=-1)\n","        img_data0.append(combined_img)\n","\n","    img_data0 = np.concatenate(img_data0, axis=2)\n","    img_data0 = np.moveaxis(img_data0, [2], [0])\n","    return np.array(img_data0).astype('float32')\n","\n","# Read and process the test images \n","# Source image pairs\n","dir_list_sc_t1 = sorted(glob.glob(os.path.join(output_path, 'T1w_1mm_padded.nii.gz')))\n","dir_list_sc_t2 = sorted(glob.glob(os.path.join(output_path, 'T2w_registered_padded.nii.gz')))\n","img_sc = image_proc_2C(dir_list_sc_t1, dir_list_sc_t2)\n","\n","# Rearrange the axes\n","img_sc = np.transpose(img_sc, (0, 3, 1, 2))\n","test_dataset = img_sc\n","test_dataset = torch.from_numpy(test_dataset).float()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AKcpxHwaoFSk"},"source":["### Load the saved model and test"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"B8C6thK5zz0I"},"source":["#### FA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8OnbatooIWt"},"outputs":[],"source":["model_FA = UNet()\n","model_FA.load_state_dict(torch.load(\"/path_to_saved_FA_model\")) \n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model_FA = model_FA.to(device)\n","model_FA.eval()\n","\n","with torch.no_grad(): \n","  test_dataset = test_dataset.to(device)\n","  output_FA = model_FA(test_dataset)\n","        \n","output_FA = output_FA.cpu()\n","output_FA = np.transpose(output_FA, (0, 2, 3, 1))\n","output_FA = output_FA.squeeze(-1)\n","output_FA = output_FA.numpy()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"44GO3L1yz1nX"},"source":["#### ADC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xw0w3LQRz3rb"},"outputs":[],"source":["model_ADC = UNet()\n","model_ADC.load_state_dict(torch.load(\"/path_to_saved_ADC_model\")) # you can either choose the (best model based on mae/best model based on loss)\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model_ADC = model_ADC.to(device)\n","model_ADC.eval()\n","\n","with torch.no_grad(): \n","  test_dataset = test_dataset.to(device)\n","  output_ADC = model_ADC(test_dataset)\n","        \n","output_ADC = output_ADC.cpu()\n","output_ADC = np.transpose(output_ADC, (0, 2, 3, 1))\n","output_ADC = output_ADC.squeeze(-1)\n","output_ADC = output_ADC.numpy()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XR9E4Uh1r6Fa"},"source":["### Reverse predicted 2D FA and ADC images back to 3D images and Unpadding synthesis FA and ADC back to (182, 218, 182)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nbDGZXhsBbi"},"outputs":[],"source":["def reverse_image_proc(image_data):\n","    \"\"\" Reverse the image processing operations.\n","    :param image_data: The processed images.\n","    :return: The original 3D image.\n","    \"\"\"\n","    # Move the axis back to its original position\n","    image_data = np.moveaxis(image_data, [0], [2])\n","\n","    return image_data\n","\n","# Create an identity affine matrix\n","affine = np.eye(4)\n","\n","# reverse to the origial 3D image\n","FA_reversed_image = reverse_image_proc(output_FA)\n","FA_reversed_image = FA_reversed_image[0:182, 0:218, 0:182]\n","FA_img_nii = nib.Nifti1Image(FA_reversed_image, affine)\n","\n","ADC_reversed_image = reverse_image_proc(output_ADC)\n","ADC_reversed_image = ADC_reversed_image[0:182, 0:218, 0:182]\n","ADC_img_nii = nib.Nifti1Image(ADC_reversed_image, affine)\n","\n","nib.save(FA_img_nii, f\"{output_path}/FA_registered_synthsized_need_resample.nii.gz\") # 192, 224, 192\n","nib.save(ADC_img_nii, f\"{output_path}/ADC_registered_synthsized_need_resample.nii.gz\") # 192, 224, 192"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"79YGltfhrmhr"},"source":["### Resample unpadded synthesis FA and ADC to original dimension (145, 174, 145) and resolution 1.25mm"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zmTb9gyErzgy"},"source":["#### First use ANTs resample method"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":150,"status":"ok","timestamp":1684007346928,"user":{"displayName":"Zhiwei GONG","userId":"02116542019368403549"},"user_tz":240},"id":"cGVaR0TBrxtL"},"outputs":[],"source":["FA_image = ants.image_read(f\"{output_path}/FA_registered_synthsized_need_resample.nii.gz\") #(182, 218, 182)\n","ADC_image = ants.image_read(f\"{output_path}/ADC_registered_synthsized_need_resample.nii.gz\") #(182, 218, 182)\n","# Resample the image to the desired resolution and size\n","FA_resampled_image = ants.resample_image(FA_image, (145, 174, 145), True, 4) \n","ADC_resampled_image = ants.resample_image(ADC_image, (145, 174, 145), True, 4) \n","\n","# Save the resampled image\n","ants.image_write(FA_resampled_image, f\"{output_path}/FA_syn_ants_resample.nii.gz\")\n","ants.image_write(ADC_resampled_image, f\"{output_path}/ADC_syn_ants_resample.nii.gz\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"w9hnjqTy2B2T"},"source":["#### Second use SimpleITK resample method "]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":140,"status":"ok","timestamp":1684007446511,"user":{"displayName":"Zhiwei GONG","userId":"02116542019368403549"},"user_tz":240},"id":"p7naUy4Q2Gyz"},"outputs":[],"source":["submitted_path = '/synthesis_submit_folder_path'\n","\n","if not os.path.exists(submitted_path):\n","  os.makedirs(submitted_path)\n","\n","FA_file_reader = sitk.ImageFileReader()\n","FA_file_reader.SetImageIO('NiftiImageIO')\n","FA_file_reader.SetFileName(f\"{output_path}/FA_syn_ants_resample.nii.gz\") # need to change path\n","FA_image = FA_file_reader.Execute()\n","\n","ADC_file_reader = sitk.ImageFileReader()\n","ADC_file_reader.SetImageIO('NiftiImageIO')\n","ADC_file_reader.SetFileName(f\"{output_path}/ADC_syn_ants_resample.nii.gz\") # need to change path\n","ADC_image = ADC_file_reader.Execute()\n","\n","FA_image.SetSpacing([1.25,1.25,1.25])\n","ADC_image.SetSpacing([1.25,1.25,1.25])\n","\n","# # Save the resampled image\n","sitk.WriteImage(FA_image, f\"{submitted_path}/FA_syn.nii.gz\")\n","sitk.WriteImage(ADC_image, f\"{submitted_path}/ADC_syn.nii.gz\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"G3on5YkN0nSs"},"source":["### Finally you should apply the normalization to get the finalized synthesis FA and ADC"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":215,"status":"ok","timestamp":1684007891354,"user":{"displayName":"Zhiwei GONG","userId":"02116542019368403549"},"user_tz":240},"id":"VqFNVya7Ydto"},"outputs":[],"source":["# Load the images\n","img_FA = nib.load(f\"{submitted_path}/FA_syn.nii.gz\")\n","img_ADC = nib.load(f\"{submitted_path}/ADC_syn.nii.gz\")\n","\n","# Convert image data to numpy array\n","data_FA = img_FA.get_fdata()\n","data_ADC = img_ADC.get_fdata()\n","\n","# Set the window and level\n","# ADC\n","ADC_window = 0.00223\n","ADC_level = 0.00111\n","\n","# FA\n","FA_window = 0.781\n","FA_level = 0.390\n","\n","# Apply the window/level\n","min_value_ADC = ADC_level - ADC_window / 2\n","max_value_ADC = ADC_level + ADC_window / 2\n","min_value_FA = FA_level - FA_window / 2\n","max_value_FA = FA_level + FA_window / 2\n","\n","# Any intensity value less than the minimum display value is set to 0,\n","# and any value greater than the maximum display value is set to 1.\n","# Values in between are rescaled to fall within the 0-1 range.\n","wl_data_ADC = np.clip((data_ADC - min_value_ADC) / (max_value_ADC - min_value_ADC), 0, 1)\n","wl_data_FA = np.clip((data_FA - min_value_FA) / (max_value_FA - min_value_FA), 0, 1)\n","\n","# Create a new NIfTI image with the windowed/leveled data and the same affine transformation\n","wl_img_ADC = nib.Nifti1Image(wl_data_ADC, img_ADC.affine)\n","wl_img_FA = nib.Nifti1Image(wl_data_FA, img_FA.affine)\n","\n","# Save the final images\n","nib.save(wl_img_ADC, f\"{submitted_path}/ADC_syn.nii.gz\")\n","nib.save(wl_img_FA, f\"{submitted_path}/FA_syn.nii.gz\")"]}],"metadata":{"colab":{"collapsed_sections":["XCKYSEIuiC3k","qPkcysPJcgDC","vcdcl2JQmpsd"],"gpuType":"V100","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
